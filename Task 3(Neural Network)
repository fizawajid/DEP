import tensorflow as tf
import numpy as np

# Loading the MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Normalizing and reshaping the datasets
x_train = x_train.reshape(x_train.shape[0], -1).T / 255.
x_test = x_test.reshape(x_test.shape[0], -1).T / 255.
y_train = y_train.T
y_test = y_test.T

# Spliting training data into training and development sets
np.random.seed(0)  # For reproducibility
shuffled_indices = np.random.permutation(x_train.shape[1])
x_train, y_train = x_train[:, shuffled_indices], y_train[shuffled_indices]

#1000 samples for the development set
x_dev = x_train[:, :1000]
y_dev = y_train[:1000]

# Remaining samples will be for the training set
x_train = x_train[:, 1000:]
y_train = y_train[1000:]

def init_params():
    np.random.seed(0)  # Ensures reproducibility
    weights_layer1 = np.random.randn(10, 784) * 0.01
    biases_layer1 = np.zeros((10, 1))
    weights_layer2 = np.random.randn(10, 10) * 0.01
    biases_layer2 = np.zeros((10, 1))
    return weights_layer1, biases_layer1, weights_layer2, biases_layer2

def ReLU(Z):
    return (Z > 0) * Z

def softmax(Z):
    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)
    return A

def forward_prop(weights_layer1, biases_layer1, weights_layer2, biases_layer2, X):
    Z1 = np.matmul(weights_layer1, X) + biases_layer1  # matmul performs dot product between weights_layer1 and X 
    activations_layer1 = ReLU(Z1)
    Z2 = np.matmul(weights_layer2, activations_layer1) + biases_layer2
    activations_layer2 = softmax(Z2)
    return Z1, activations_layer1, Z2, activations_layer2

def ReLU_deriv(Z):
    return np.where(Z > 0, 1, 0)

def one_hot(Y):
    num_classes = np.max(Y) + 1
    one_hot_Y = np.eye(num_classes)[Y].T
    return one_hot_Y

def backward_prop(Z1, activations_layer1, Z2, activations_layer2, weights_layer1, weights_layer2, X, Y):
    one_hot_Y = one_hot(Y)
    dZ2 = activations_layer2 - one_hot_Y
    gradient_weights_layer2 = 1 / X.shape[1] * dZ2.dot(activations_layer1.T)
    db2 = 1 / X.shape[1] * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = weights_layer2.T.dot(dZ2) * ReLU_deriv(Z1)
    gradient_weights_layer1 = 1 / X.shape[1] * dZ1.dot(X.T)
    db1 = 1 / X.shape[1] * np.sum(dZ1, axis=1, keepdims=True)
    return gradient_weights_layer1, db1, gradient_weights_layer2, db2

def update_params(weights_layer1, biases_layer1, weights_layer2, biases_layer2, gradient_weights_layer1, db1, gradient_weights_layer2, db2, alpha):
    weights_layer1 -= alpha * gradient_weights_layer1
    biases_layer1 -= alpha * db1    
    weights_layer2 -= alpha * gradient_weights_layer2  
    biases_layer2 -= alpha * db2    
    return weights_layer1, biases_layer1, weights_layer2, biases_layer2

def get_predictions(activations_layer2):
    return np.argmax(activations_layer2, axis=0)

def is_accurate(predictions, Y):
    return np.sum(predictions == Y) / Y.size

def gradient_descent(X, Y, alpha, iterations):
    weights_layer1, biases_layer1, weights_layer2, biases_layer2 = init_params()
    for i in range(iterations):
        Z1, activations_layer1, Z2, activations_layer2 = forward_prop(weights_layer1, biases_layer1, weights_layer2, biases_layer2, X)
        gradient_weights_layer1, db1, gradient_weights_layer2, db2 = backward_prop(Z1, activations_layer1, Z2, activations_layer2, weights_layer1, weights_layer2, X, Y)
        weights_layer1, biases_layer1, weights_layer2, biases_layer2 = update_params(weights_layer1, biases_layer1, weights_layer2, biases_layer2, gradient_weights_layer1, db1, gradient_weights_layer2, db2, alpha)
        if i % 10 == 0:
            print(f"Iteration: {i}")
            predictions = get_predictions(activations_layer2)
            print(f"Accuracy: {is_accurate(predictions, Y)}")
    return weights_layer1, biases_layer1, weights_layer2, biases_layer2

# Training the model
weights_layer1, biases_layer1, weights_layer2, biases_layer2 = gradient_descent(x_train, y_train, 0.10, 500)

def predict(X, weights_layer1, biases_layer1, weights_layer2, biases_layer2):
    _, _, _, activations_layer2 = forward_prop(weights_layer1, biases_layer1, weights_layer2, biases_layer2, X)
    return np.argmax(activations_layer2, axis=0)

def predict_test(index, weights_layer1, biases_layer1, weights_layer2, biases_layer2):
    current_image = x_train[:, index, None]
    prediction = predict(current_image, weights_layer1, biases_layer1, weights_layer2, biases_layer2)
    label = y_train[index]
    print(f"Prediction: {prediction}")
    print(f"Label: {label}")

# Test predictions on a few examples
predict_test(0, weights_layer1, biases_layer1, weights_layer2, biases_layer2)
predict_test(1, weights_layer1, biases_layer1, weights_layer2, biases_layer2)
predict_test(2, weights_layer1, biases_layer1, weights_layer2, biases_layer2)
predict_test(3, weights_layer1, biases_layer1, weights_layer2, biases_layer2)

# Evaluating accuracy on the development set
dev_predictions = predict(x_dev, weights_layer1, biases_layer1, weights_layer2, biases_layer2)
print(f"Development Set Accuracy: {is_accurate(dev_predictions, y_dev)}")

# Evaluating accuracy on the test set
test_predictions = predict(x_test, weights_layer1, biases_layer1, weights_layer2, biases_layer2)
print(f"Test Set Accuracy: {is_accurate(test_predictions, y_test)}")
